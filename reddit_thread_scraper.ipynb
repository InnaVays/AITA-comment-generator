{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created).strftime('%d-%m-%Y')\n",
    "\n",
    "# set initial parameters\n",
    "headers = {'User-agent': 'Apple-Air AITA scraper Weird Opportunity and some other stuff for unique user agent'}\n",
    "url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "\n",
    "data = []\n",
    "data_empt = []\n",
    "\n",
    "thread_name = 'AmItheAsshole'\n",
    "params = {'subreddit': thread_name, 'size': 500}\n",
    "\n",
    "latest_posts_num = 15000\n",
    "# retrieve 15000 lateest post from pushshift api\n",
    "while len(data) < latest_posts_num:\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    time.sleep(1)\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        posts = json_data['data']\n",
    "        for post in posts:\n",
    "            # check for duplicates\n",
    "            if post['id'] not in [d['id'] for d in data]:  \n",
    "                post_data = {}\n",
    "                post_data['title'] = post['title']\n",
    "                # post text\n",
    "                post_data['text'] = post['selftext']\n",
    "                post_data['date'] = get_date(post['created_utc'])\n",
    "                post_data['num_upvotes'] = post['score']\n",
    "                post_data['num_comments'] = post['num_comments']\n",
    "                post_data['id'] = post['id']\n",
    "                \n",
    "                # retrive data from OLD reddit to get the most popular comment\n",
    "                if post_data['text'] != '[removed]':\n",
    "                    comments_url = f\"https://old.reddit.com/r/{thread_name}/comments/{post_data['id']}/\"\n",
    "                    response = requests.get(comments_url, headers=headers)\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    entry_div = soup.find_all('div', class_=\"entry unvoted\")\n",
    "                    if len(entry_div) > 3:\n",
    "                        # text score\n",
    "                        post_upvotes = soup.find_all(class_=\"score unvoted\")[0]\n",
    "                        if post_upvotes != None:\n",
    "                            post_data['num_upvotes'] = post_upvotes.get('title')\n",
    "                        else:\n",
    "                            post_data['num_upvotes'] = None\n",
    "                        # first comment\n",
    "                        first_comment = entry_div[2].find(action=\"#\", class_=\"usertext warn-on-unload\")\n",
    "                        if first_comment != None:\n",
    "                            post_data['first_comment'] =first_comment.text\n",
    "                        else:\n",
    "                            post_data['first_comment_upvotes'] = None\n",
    "                        #first comment score\n",
    "                        first_comment_upvotes = entry_div[2].find(class_=\"score unvoted\")\n",
    "                        if first_comment_upvotes != None:\n",
    "                            post_data['first_comment_upvotes'] = first_comment_upvotes.get('title')\n",
    "                        else:\n",
    "                            post_data['first_comment_upvotes'] = None\n",
    "                        data.append(post_data)\n",
    "                        # save copy (connection crushes a lot)                        \n",
    "                        if len(data) % 500 == 0:\n",
    "                            df = pd.DataFrame(data)\n",
    "                            df.to_csv(f'AITA_posts_{len(data)}.csv', index=False)\n",
    "                            print(f\"{len(data)} posts saved to CSV file.\")\n",
    "        \n",
    "        # update time parameter for next iteration. UTC format\n",
    "        params['before'] = posts[-1]['created_utc']\n",
    "        # set time delay. Reddit cap is 60 request per minute\n",
    "        time.sleep(1)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error {response.status_code}.\")\n",
    "        break\n",
    "\n",
    "# save final data to CSV file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('AITA_posts.csv', index=False)\n",
    "print(f\"{len(data)} posts saved to CSV file.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
